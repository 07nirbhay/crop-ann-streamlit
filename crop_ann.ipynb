{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "th7lRKW98IJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awaw_ND_1h2d"
      },
      "outputs": [],
      "source": [
        "!pip -q install pandas numpy scikit-learn tensorflow matplotlib joblib streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "up = files.upload()"
      ],
      "metadata": {
        "id": "gHz1wpCZ7NEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'Crop_recommendation.csv'\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"/content/artifacts\")\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "ExAUMjVg8Ehv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scikit-learn joblib gradio==4.44.0\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "sY3R3htD8YSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATA_PATH).drop_duplicates().dropna()\n",
        "\n",
        "FEATURES = [\"N\", \"P\", \"K\", \"temperature\", \"humidity\", \"ph\", \"rainfall\"]\n",
        "TARGET = \"label\"\n",
        "\n",
        "X = df[FEATURES].values.astype(\"float32\")\n",
        "y_text = df[TARGET].values"
      ],
      "metadata": {
        "id": "qOdCGhsF8p4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_text)\n",
        "num_classes = len(le.classes_)\n",
        "y_cat = utils.to_categorical(y, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "hhFJo8lE9KNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_cat, test_size=0.30, random_state=42, stratify=y_cat\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "vH4WJbfv9TSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "tOM1aOq79Vrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.arange(num_classes),\n",
        "    y=y_train_int\n",
        ")\n",
        "class_weights = {i: w for i, w in enumerate(class_weights)}"
      ],
      "metadata": {
        "id": "cieY6Pwk9YdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model(X_train_s.shape[1], num_classes)"
      ],
      "metadata": {
        "id": "9xXjo09y9bsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15, restore_best_weights=True)\n",
        "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-5)\n",
        "ckpt_path = ARTIFACTS_DIR / \"best_model.keras\"\n",
        "mc = callbacks.ModelCheckpoint(filepath=str(ckpt_path), monitor=\"val_accuracy\", save_best_only=True)\n",
        "\n",
        "# ---- Train\n",
        "history = model.fit(\n",
        "    X_train_s, y_train,\n",
        "    validation_data=(X_val_s, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[es, rlr, mc],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "5J0NAFJR9geQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tf.keras.models.load_model(ckpt_path)\n",
        "y_pred_probs = best_model.predict(X_test_s)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "print(f\"\\nTest Accuracy: {(y_true == y_pred).mean():.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "iMfj-mrn9l3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, ARTIFACTS_DIR / \"scaler.joblib\")\n",
        "with open(ARTIFACTS_DIR / \"label_encoder_classes.json\", \"w\") as f:\n",
        "    json.dump(le.classes_.tolist(), f)\n",
        "print(\"\\nSaved:\")\n",
        "print(f\"- Model: {ckpt_path}\")\n",
        "print(f\"- Scaler: {ARTIFACTS_DIR / 'scaler.joblib'}\")\n",
        "print(f\"- Label classes: {ARTIFACTS_DIR / 'label_encoder_classes.json'}\")"
      ],
      "metadata": {
        "id": "Zn8eMTTo9x4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, json, joblib, tensorflow as tf\n",
        "\n",
        "best_model = tf.keras.models.load_model(\"/content/artifacts/best_model.keras\")\n",
        "scaler = joblib.load(\"/content/artifacts/scaler.joblib\")\n",
        "classes = json.loads(open(\"/content/artifacts/label_encoder_classes.json\").read())\n",
        "\n",
        "FEATURES = [\"N\",\"P\",\"K\",\"temperature\",\"humidity\",\"ph\",\"rainfall\"]\n",
        "\n",
        "sample = {\n",
        "    \"N\":90, \"P\":42, \"K\":43,\n",
        "    \"temperature\":26.5, \"humidity\":80.0, \"ph\":6.5, \"rainfall\":200.0\n",
        "}\n",
        "x = np.array([[sample[f] for f in FEATURES]], dtype=\"float32\")\n",
        "x_s = scaler.transform(x)\n",
        "probs = best_model.predict(x_s)[0]\n",
        "pred = classes[int(np.argmax(probs))]\n",
        "pred, float(np.max(probs))\n"
      ],
      "metadata": {
        "id": "a1visX9t92tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import json, joblib, numpy as np, streamlit as st, tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "ARTIFACTS = Path(\"artifacts\")\n",
        "MODEL = tf.keras.models.load_model(ARTIFACTS / \"best_model.keras\")\n",
        "SCALER = joblib.load(ARTIFACTS / \"scaler.joblib\")\n",
        "CLASSES = json.loads((ARTIFACTS / \"label_encoder_classes.json\").read_text())\n",
        "\n",
        "FEATURES = [\"N\",\"P\",\"K\",\"temperature\",\"humidity\",\"ph\",\"rainfall\"]\n",
        "\n",
        "st.title(\"ðŸŒ¾ ANN Crop Recommendation\")\n",
        "st.write(\"Enter soil and weather parameters to get a recommended crop.\")\n",
        "\n",
        "cols = st.columns(7)\n",
        "vals = {}\n",
        "vals[\"N\"] = cols[0].number_input(\"N (Nitrogen)\", 0.0, 200.0, 90.0)\n",
        "vals[\"P\"] = cols[1].number_input(\"P (Phosphorus)\", 0.0, 200.0, 42.0)\n",
        "vals[\"K\"] = cols[2].number_input(\"K (Potassium)\", 0.0, 200.0, 43.0)\n",
        "vals[\"temperature\"] = cols[3].number_input(\"Temperature (Â°C)\", -5.0, 55.0, 26.5)\n",
        "vals[\"humidity\"] = cols[4].number_input(\"Humidity (%)\", 0.0, 100.0, 80.0)\n",
        "vals[\"ph\"] = cols[5].number_input(\"Soil pH\", 0.0, 14.0, 6.5)\n",
        "vals[\"rainfall\"] = cols[6].number_input(\"Rainfall (mm)\", 0.0, 500.0, 200.0)\n",
        "\n",
        "if st.button(\"Recommend Crop\"):\n",
        "    x = np.array([[vals[f] for f in FEATURES]], dtype=\"float32\")\n",
        "    x_s = SCALER.transform(x)\n",
        "    probs = MODEL.predict(x_s)[0]\n",
        "    idx = int(np.argmax(probs))\n",
        "    st.success(f\"âœ… Recommended crop: **{CLASSES[idx]}**\")\n",
        "    st.write(\"Class probabilities:\")\n",
        "    st.json({c: float(p) for c, p in zip(CLASSES, probs)})\n",
        "\n",
        "st.caption(\"Model: Dense ANN | Optimizer: Adam | Loss: Categorical Crossentropy\")\n"
      ],
      "metadata": {
        "id": "YTHX2CY2-Aue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/requirements.txt\n",
        "streamlit\n",
        "tensorflow==2.17.0\n",
        "scikit-learn\n",
        "pandas\n",
        "numpy\n",
        "joblib"
      ],
      "metadata": {
        "id": "z8gR_dG4-EDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, zipfile, pathlib\n",
        "\n",
        "project_dir = \"/content/crop_ann_streamlit\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "shutil.copy(\"/content/app.py\", f\"{project_dir}/app.py\")\n",
        "shutil.copy(\"/content/requirements.txt\", f\"{project_dir}/requirements.txt\")\n",
        "\n",
        "# copy artifacts\n",
        "shutil.copytree(\"/content/artifacts\", f\"{project_dir}/artifacts\", dirs_exist_ok=True)\n",
        "\n",
        "# optional: add README\n",
        "with open(f\"{project_dir}/README.md\",\"w\") as f:\n",
        "    f.write(\"# ANN Crop Recommendation (Streamlit)\\n\\nRun with: `streamlit run app.py`\")\n",
        "\n",
        "# zip it\n",
        "zip_path = \"/content/crop_ann_streamlit.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for p in pathlib.Path(project_dir).rglob(\"*\"):\n",
        "        z.write(p, arcname=p.relative_to(project_dir))\n",
        "print(\"Zipped at:\", zip_path)"
      ],
      "metadata": {
        "id": "VF2hG90q-KGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Co8WsPa1-P7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}